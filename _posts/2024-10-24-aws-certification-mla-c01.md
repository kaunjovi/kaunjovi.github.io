---
layout: post
title: AWS Certifications MLA C01
categories: [certifications] 
---


## 2025 certifications

1. Do they have certificate? Are they free? 

1. Generative ai â€” A way of life https://www.analyticsvidhya.com
1. Introduction to generative ai https://www.cloud.google.com
    1. https://www.cloudskillsboost.google/course_templates/536

1. Generative ai for everyone https://www.deeplearning.ai
1. Generative ai for beginners https://github.com/microsoft
1. Generative ai explained https://www.nvidia.com
1. Getting started with large language models https://www.analyticsvidhya.com
1. Generative ai with large language models https://aws.amazon.com
1. Building llm applications using prompt engineering https://www.independentcourseprovider.com
1. Generative ai business transformation https://www.ibm.com
1. Generative ai for data scientists specialization https://www.dataspecializationplatform.com

## 2025 certifications / Paid 

1. AWS certifications


## Course Notes : Introduction to Generative AI by Google 

1. https://www.cloudskillsboost.google/course_templates/536/video/520739
1. Roger Martinez @ LinkedIn 4m Google is Developer Relationship 
1. What is AI? 
    1. AI is a discipline e.g. Physics is a discipline of Science 
    1. Theory and Methods to enable machines to think like Humans. 
1. What is the difference between AI and ML 
    1. Sub field of AI. 
    1. Way to train machine / models with data 
    1. The trained model then can work on new data and make independent decisions based on whatever is learned. 
    1. Computers can learn - using ML - without explicit programming. 
1. What are the different types of ML 
    1. Supervised - learns from labeled data 
        1. Predict based on past data. 
        1. How much did people tip in the past? How much will they? 
        1. What are the factors that impacted? How can we increase the tip amount? 

    1. Unsupervised - from unlabled data 
        1. Raw, unlabled data. Discover trends from data. 

1. GenAI = AI to generate content - including synthetic data - how to use that in testing and Test Data Mgmt. 

1. **Machine Learning**
1. Unsupervised 
    1. Input Data X > Model > New input to Model > Some output, take it or leave it. 
1. Supervised 
    1. Input data X > Model > Learned now > Test input y > Test output x > Is test output x and the actual output X close? 
    1. If no, go back to the Model and optimize. 

1. **Deep Learning as a subset of ML** 
1. Deep Learning is a type of ML that uses artificial Neural Networks 
1. Neural networks can use both labled and unlabled data. This is called **semi-supervised** learning 
1. Trained on a small amount of labled data and a large amount of unlabled data. 

1. Deep Learning Models are of two types - Generative and Discriminative. 

1. **Discriminative** models - Discriminates between different kind of data instances.  
    1. used to classify or predict lables. Give them labled data points 
    1. they learn the relationship between features and lables 
    1. they look at the conditional probability distribution 
    1. Given X (input) what is the probablity of Y (output)
    1. Give a pictture, what is the probability it is a Dog 

1. **Generative** models - generates new data 
    1. It learns from probability distribution of existing data 
    1. 

1. **GenAI is a subset of Deep Learning** 


1. **LLM is a also a subset of Deep Learning** 







## AWS certifications / What is available @ Q4 2024

1. [AWS certification](https://aws.amazon.com/certification/)

1. Foundational - No prior experience needed.
    1. Cloud Practitioner 
    1. [AWS Certified AI Practitioner (AIF-C01) Exam Guide](https://d1.awsstatic.com/training-and-certification/docs-ai-practitioner/AWS-Certified-AI-Practitioner_Exam-Guide.pdf?p=cert&c=ai&z=3)
1. Associate
    1. ML Engineer 
    1. Data Engineer 
    1. *AWS Certified Solutions Architect - Associate (SAA-C03)*
    1. Developer, SysOps Administrator 
1. Professional - 2 years of prior AWS Cloud experience recommended.
    1. Solution Architect / Professional
    1. DevOps Engineer 
1. Speciality 
    1. machine Learning 
    1. Security, Advanced Network 


## Practice Exams 
1. [Exam Prep Official Practice Question Set: AWS Certified Machine Learning Engineer - Associate (MLA-C01 - English)](https://explore.skillbuilder.aws/learn/course/external/view/elearning/19688/exam-prep-official-practice-question-set-aws-certified-machine-learning-engineer-associate-mla-c01-english)

## Types of questions to expect in exam 

1. Gen AI and Bedrock feature a lot but not too deep on different models and choosing them
1. a LOT of focus on SageMaker and its features (Model REgistry, debugger, model monitor, data wrangler, clarify, feature store)
1. MLOps in general
1. Cost optimization questions
1. dont need deep AI knowledge (deep learning, fitting, algorithms etc) - so its more a focus on AWS servies
1. it is an associate cert so not at specialty level
1. you have to manage time well - its 85 questions - if you have ESL+30 that may help
1. Stephane Maarek's course is okay but its early days and a bit rough and doesnt match the exam domains yet (
1. since the course came out before the Exam guide was published)
1. Skillbuilder was okay
1. Expect normal AWS questions as well on VPC etc - since you are setting up AWS services
1. knowledge of otehr AI services (rekognition / transcribe / comprehend / S3 etc) at basic level



1. [MLA C01 Exam Guide](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45320497#notes)

1. Exam about details of the components. Not about designing and architecting. We will need to get to that as well. Soon. 
1. Question types 
    1. Multi choice - one out of 4. 
    1. Multi response - many out of 5 or more.  
    1. Ordering - 3 to 5 correct. and correct ordering. 
    1. Matching - 3 to 7. 
    1. Case study - one case study. multiple questions based on that. 
    1. No negative marking - guess please. Prep so you dont have to. 
1. 1000 - 720 passing marks 
1. Domains 
    1. Data preperation - 28% 
    1. ML model dev - 26% 
    1. Deployment and orch - 22% 
    1. monitoring, maintenance, security - 24% 
1. Take p/o of the course, and those that we need twitter update for. 

1. **more material** if you have time
1. [SageMaker Developer guide](https://docs.aws.amazon.com/sagemaker/) 
    1. [pdf](https://gmoein.github.io/files/Amazon%20SageMaker.pdf)

1. 3 hours. 65 questions (85 during beta)
1. flag the ones that you want to come back and recheck 




1. This one is for SageMaker. Is there any for Domnio??? What are the other alternatives??? 
1. What is the latest thinking about MLOps ???
1. [Udemy / Master MLA-C01](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45284683#overview)

## Data Ingestion and Storage 

1. Amazon Kinesis (think Kafka)
1. Amazon FSx 
1. Amazon S3 
1. Amazon Elastic 
    1. Block Store (EBS)
    1. File System (EFS)

## Data Types and Formats 

## Data Warehouse vs. Lakes vs. Lakehouse 

## Extract Transform and Load (ETL) pipelines and Orchestration 

## AWS services / storage 
## AWS services / streaming 

1. Elastic MapReduce (EMR) - running big parallel data processing jobs at scale. Hadoop. Spark. 


## Data / Missing. Unbalanced. Outlier. 

## Data / Transformation 

1. To Transform data to the format needed by the ML to work. 

## SageMaker / Data processing and analysis 

1. built in algorithms you can just use. XGBoost. LinearLearner. 


## AWS Glue / Data processing and Transformation. 

## Amazon Rekognition 

1. Image recognition. 

## Amazon Forecast 

1. Forecasting data. 

## Amazon Q 

1. For building chatbot 


## Model Training, Tuning and Evaluation. 

1. Deep learning fundamentals. How does Neural Network work. What is the meaning of Topology. 
1. Measuring Model Performance. 
1. Automatic Model Tuning in Sagemaker

## Generative AI. Transformer Architecture. 

1. Transformer Architecture. 
1. Self Attention. 
1. How GPT works. How Claude works. 
1. SageMaker Jumpstart. 

## Amazon Bedrock 

1. Built on top of Foundation models. 
1. Retrieval Augmented Generation (RAG) works - preventing hallucinations. injecting proprietary data. 
1. Knowledge Bases - store the external data 
1. Vector Stores 
1. Guardrails - prevent model from doing things that you dont want it to do. 
1. LLM agents - custom code to 
1. 

## ML operations (MLOps)

1. SageMaker 
1. Containerize (ECS, ECR)
1. Cloudformation 
1. CDK
1. CodeDeploy 
1. CodeBuild 
1. CodePipeline 
1. EventBridge 
1. Step Functions
1. Managed Workflows for Apache Airflow (MWAA)

## Security. Identity. Compliance. 

1. Securing Data . in SageMaker
1. AWS IAM 
1. KMS 
1. Macie 
1. Secrets Manager 
1. Website Application Firewall (WAF)
1. Shield 
1. Network isolation - VPC, Private Link 

## Management. Governance. 

1. CloudWatch. CloudFormation. Config. CloudTrail. 
1. Budgets. CostExplorer. 
1. New - TrustedAdvisor. X-Ray 

## [Data Engineering fundamentals](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45284653#overview)

## Three types of Data. 3 Musketeers

1. Structured data 
    1. We know the columns, the data types in them, and their relations with each other. 
    1. Easy to query. 
    1. There is a defined schema. 
    1. The data is cleaned and consistent. 
    1. examples - Database tables (Oracle, Redshift, MySQL, ...) , CSV files, XLS etc. 
1. Unstructured data 
    1. No defined structure or schema. 
    1. Can't query because there is no structure. 
    1. Video, audio, images, emails, word file, pdf files, text without a fixed format. 
1. Semi structured data 
    1. There is no strict schema or structure, but there is some tags, hierarchies, or other patterns. 
    1. XML, JSON, email headers, Log files of various formats. 

## Properties of data. 3 Vs. 

1. Volume - How much data. GB, PB, TB. 
    1. How much do we have in EDC and in UDS??? 
    1. How much data comes in per day? How much are we keeping? 
    1. How long for? How much in transactional storage? How much in backup?
1. Velocity - the speed at which new data is generated, collected and processed. 
    1. Batch vs. stream in real time and process continuously. 
    1. High velocity? Real time or near real time processing capability is required. 
    1. Real time vs. Near real time ???
    1. Kinesis data stream vs. Kinesis data firehose ???
    1. Sensor data from IOT - 
    1. High frequency trading system - 
1. Variety - Types of data  - structured, semi structured, Unstructured
    1. 
1. Veracity 

## [DataWarehouse vs. DataLakes](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45284683#overview)

1. DataWarehouse - legacy approach to data engineering. 
    1. Centrailized. Optimized for analysis. 
    1. Data collected from various sources. Arranged in structured format. 
    1. Optimized for analysis. Optimized for read heavy operations. 
    1. Star schema vs Snowflake schema. 
    1. Write vs Read heavy operations??
    1. Amazon Redshift, Google BigQuery, Azure SQL Data Warehouse (Amazon used Oracle - very expensive - before Redshift)
    1. Source1, Source2, Source3, ... -> DataWarehouse -> DataMart1, DataMart2, DataMart3, ... 

1. Data Lake 
    1. Holds raw data in native format. Could be structured, semi-structured, or Unstructured.  
        1. No replication is happening in any structured format. 
    1. Amazon S3, Hadoop Distributed File System (HDFS), Azure Data Lake Storage 
    1. S3 (keep raw data ) > Glue (extract meta data from the raw data ) > Athena (use the meta data to query the raw data)

1. DataWarehouse vs DataLake
    1. Schema on Write vs Schema on read 
        1. DataWarehouse has to write to a schema. Hence ETL. 
        1. DataLake used schema on write. Hence ELT. 
    1. Structured vs Semi/un structured 
        1. DataWarehouse is primarily used for structured data 
        1. DataLake are primarily suitable for Semi/un structured data. 
    1. Agility 
        1. DataWarehouse - less agile to ingest 
        1. DataLake - more agile to ingest 
    1. Cost 
        1. DataWarehouse - more 
        1. DataLake - less 

1. When to choose which one? 
    1. DataWarehouse 
        1. Structured data sources 
        1. BI and analytics is the primary use 
        1. Need fast query. complex query 
            1. Know the schema. Know the indices. Can optimize. 
        1. Know the data from different sources. Data integration is done. 
            1. Data is kept in a read optimized way. 
    1. DataLake
        1. You have variety of data ./semi/un structured data. 
        1. Storing data in a cost effective way is the primary objective. 
        1. You are unsure about what needs to be done with the data later and want to be flexible about it. 
        1. Data discovery, Advanced Analytics, ML are the probable use cases. 

1. [DataLakehouse](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45284683#notes)
    1. Databricks concept 
    1. Supports all 3 varieties of data. 
    1. Support 2 varieties of schema on write and on ready
    1. Supports both families of use case Analytics and BI  and data discovery, AI/ML 
    1. S3 > Delta Lake - brings ACID to big data. 
    1. AWS Lake Formation - with S3 and Redshift Spectrum (to query that data)
    1. Delta Lake - opensource storage layer, brings ACDI transactions to Apache Spark and big data 

1. [Data Mesh](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45337673#notes)
    1. Coined in 2019 by who ?? 
    1. More about governance and organization of data within an enterprise. 
    1. Domain based Data Management 
    1. Central standards. Federated governance.
        1. Can AWS Glue be the centralized catalog for all the data products for users to discover???
    1. Self service tooling and infr for data owners to leverage to create and expose data products. 

1. [ETL pipelines](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/lecture/45284695#notes)
    1. Real time or batches
    1. Database, files, APIs, message queues etc. 
    1. Data integrity
        1. What if api fails on call 
        1. Do you retry it. How often. How many times. 
    1. Velocity - in real time, in near real time (in a few minutes), batch fashion (overnight)

1. Transform 
    1. Transform the raw data into a suitable forrmat for the target data warehouse 
    1. Cleanse the data - duplicate data, fixing errors 
    1. Correct the format - incorrect format 
    1. Enrich - add additional data / reference data to the raw data 
    1. Aggregation or computations - totals, sums, averages, means etc. 
    1. Encoding / decoding - zipped data needs to be unzipped, some data needs to be encrypted 
    1. Handling missing values - drop rows with missing values ? 

1. Load 
    1. Batch vs realtime 
    1. reliable. if something goes wrong we need to know about it and be able to do something about it. 

1. Managing ETL pipelines 
    1. Automated. Reliable way. 
    1. AWS glue can do the job 
    1. Orchestration 
        1. Glue Workflows
        1. EventBridge
        1. Managed Workflows for Apache Airflow (MWAA)
        1. AWS step functions 
        1. Lambda 

## Data Sources 

1. JDBC - Platform independent. Java dependent. 
1. ODBC - Not in Java? No problem. Use ODBC. Platform dependent. You will need Platform specific drivers. 
1. Raw files 
1. APIs 
1. Streams (Kafka, Kinesis) 

## Data Formats 

1. CSV - Text based. Human readable. Small to medium sized datasets only please. Not very efficient about storage. 
1. Javascript Object Notation (JSON) - Structured or semi-structured data, between backend services and front end. 
    1. Light weight. Text based. Human Readable. 
    1. Based on key value pairs. 
    1. Main diffrence - it can be semi-structured. Flexible schema. Nested structures. 
    1. Good for Configurations and settings 
    1. Used in : NoSQL databases e.g. MongoDB. RESTful APIs. 
1. Avro - Binary format. Both data and schema. Machines can read, without knowing the origination. 
    1. Not human readable / writable. 
    1. Schema is there in the file. So if the schema was changing in the source, distribution system, that is no problem. 
    1. Adding schema to data everytime means some extra space. But gives the flexibility of handling changing schema. 
    1. efficient serialization for data transport. 
    1. Usage : Apache Kafka, Sparc, Flink and Hadoop ecosystem
1. Parquet - Columnar storage. Optimized for analytics. You might be looking for only some columns, not all, for analytics. 
    1. Storage optimization. You can put entire set of columns in a different server. How does that help ??? 
    1. Usage : Redshift Spectrum, Hadoop, Apache Spark / Hive / Impala


## Investigate Further ??? 

1. Redishift Spectrum ??? 

