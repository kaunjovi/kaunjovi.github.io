---
layout: post
title: RAG
categories: [RAG, LLM, GenAI] 
---





## Cache-Augmented Generation (CAG)

1. https://medium.com/@jagadeesan.ganesh/cache-augmented-generation-cag-a-rising-competitor-to-rag-5bb70428f8d8



## Apache Spark / How does it work? 

1. AWS Glue is based on Apache Spark. 
1. until an action called there will not be any actual execution


## AWS Glue 

1. https://docs.aws.amazon.com/glue/

1. AWS Glue is a **serverless data integration** service 
1. integrate data from multiple sources. 
1. It also includes additional productivity and data ops tooling for authoring, running jobs, and implementing business workflows.
1. With AWS Glue, you can discover and connect to more than 
1. 70 diverse data sources and manage your data in a **centralized data catalog**. 
1. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines 
1. to load data into your data lakes. 
1. immediately search and query cataloged data using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum.
1. AWS Glue consolidates major data integration capabilities into a single service. 
1. These include data discovery, modern ETL, cleansing, transforming, and centralized cataloging. 
1. It's also serverless, which means there's no infrastructure to manage. 
1. With flexible support for all workloads like ETL, ELT, and streaming in one service, AWS Glue supports users across various workloads and types of users.
1. Also, AWS Glue makes it easy to integrate data across your architecture. 
1. It integrates with AWS analytics services and Amazon S3 data lakes. 
1. AWS Glue has integration interfaces and job-authoring tools that are easy to use for 

1. ability to scale on demand, 
1. AWS Glue helps you focus on high-value activities that maximize the value of your data. 
1. It scales for any data size, and supports all data types and schema variances. 
1. To increase agility and optimize costs, AWS Glue provides built-in high availability and pay-as-you-go billing.

1. With AWS Glue, you pay an 
1. hourly rate, billed by the second, 
1. for crawlers (discovering data) and extract, transform, and load (ETL) jobs (processing and loading data). 
1. For the AWS Glue Data Catalog, you pay a simplified monthly fee for storing and accessing the metadata. 
1. The first million objects stored are free, and the first million accesses are free. 
1. a development endpoint to interactively develop your ETL code, 
1. you pay an hourly rate, billed per second. 
1. For AWS Glue DataBrew, the interactive sessions are billed per session, and DataBrew jobs are billed per minute. 
1. Usage of the AWS Glue Schema Registry is offered at no additional charge.


https://vikramanand.ewebinar.com/webinar/17548/join/9631324

## Apache Iceberg vs Apache Hudi vs Delta Lake 

1. [Apache Iceberg Vs. Delta Lake Vs. Apache Hudi! Data Lake Storage Solutions Compared!](https://www.youtube.com/watch?v=PT6I81JHWxU)

1. **Table format**
1. Iceberg - Open table format 
1. Delta table format 
1. Hudi 




## Car accessories / wireless carplay 

1. Not many returnable 
1. Complaints of disconnecting when car goes over patchy road.
1. Also complaints of lag, or functionalities being tempered with e.g. call reception etc. 

1. Returnable 
1. 2750 - Skadioo Wireless CarPlay Adapter Convert Wired Carplay/Android auto Dongle to Wireless | Plug & Play Wireless Car Connectivity, Easy Installation, Fast Connection, No Wiring Needed.
1. 3049 - HB PLUS 2-in-1 Wireless CarPlay Adapter for iPhone/Android Convert Wired to Wireless Apple CarPlay Adapter, Mini Size with Extension Cable USB & USB-C/Type-C, Stable Control & No Delay, Black

1. Replacement 
1. 4699 - MSXTTLY Wireless Carplay Wireless Android Auto Adpater, SMT-AC01 for Cars with Wired Carplay & Android Auto, Support 5.8 Ghz WiFi & 5.2 Bluetooth, USB C/A Mini & Stable
1. 4899 - CARLINKIT 5.0 Wireless Adapter for CarPlay/Android Auto - Ultimate Performance: Unbeatable Stability, Low Latency, Siri/Google Assistant, Compatible with Wired CarPlay/Android Auto Vehicles
1. 4605 - Carlizem Latest 2025 Wireless Carplay and Android Auto Adapter 2-in-1 | Quick Connection & Auto Reconnect | Bluetooth 5.3 & 5.8Ghz WiFi| High Performance | Easy to Use & Setup iOS & Android







## Data Catalog 

1. [What is a Data Catalog and Why Are They Useful? ](https://www.youtube.com/watch?v=rRM0pMkLN_I)

## [Create Apache Iceberg Tables using AWS Glue](https://www.youtube.com/watch?v=o8GffXE1tpo)

1. 


## AWS Glue Data Catalog 

1. [Populating the AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/populate-catalog-methods.html)


1. AWS Glue Data Catalog (MetaStore ??)
    1. S3, DynamoDb, on premises databases e.g. MySQL etc. 
    1. MetaData that it crawls out of databases 
    1. Location. Table. Connetion information. 
    1. Table. Underlying file format etc. 
    1. Once you have that - you can use ETL services - Athena, Glue ETL. 
    1. [AWS Glue Crawler [AWS Console 2023 Full Demo]](https://www.youtube.com/watch?v=L8Uwh3U3s9Y)


1. How to populate AWS Glue Data Catalog:
1. AWS Glue crawler – An AWS Glue crawler can automatically discover and catalog data sources like databases, data lakes, and streaming data. The crawlers are the most common and recommended method to populate the Data Catalog as they can automatically discover and infer metadata for a wide variety of data sources.
1. Integrating with other AWS services – You can populate the Data Catalog with metadata from services like AWS Lake Formation and Amazon Athena. These services can discover and register data sources in the Data Catalog.
1. Populating from an existing metadata repository – If you have an existing metadata store like Apache Hive Metastore, you can use AWS Glue to import that metadata into the Data Catalog. For more information, see Migration between the Hive Metastore and the AWS Glue Data Catalog on GitHub.
1. Manually adding metadata – You can manually define databases, tables, and connection details and add them to the Data Catalog using the AWS Glue console, Lake Formation console, AWS CLI, or AWS Glue APIs. Manual entry is useful when you want to catalog data sources that cannot be crawled.
1. 



## Glue job / How to debug? 

1. https://stackoverflow.com/questions/68781443/how-to-debug-an-aws-glue-pyspark-job
1. AWS Glue is based on Apache Spark
1. [Debugging demanding stages and straggler tasks](https://docs.aws.amazon.com/glue/latest/dg/monitor-profile-debug-straggler.html)


## PySpark For AWS Glue Tutorial

- https://www.youtube.com/watch?v=DICsZiwuHJo

## AWS Glue Tutorial for Beginners

1. [AWS Glue Tutorial for Beginners [NEW 2024 - FULL COURSE]](https://www.youtube.com/watch?v=ZvJSaioPYyo)
1. Fully managed ETL service 


## [AWS re:Invent 2022 - How Disney used AWS Glue as a data integration and ETL framework (ANT335)](https://www.youtube.com/watch?v=ce6t3FqB_Z4)
1. AWS Glue—a serverless data integration service 
1. as a key component to replace thousands of Apache Hadoop, Spark, and Sqoop jobs. 
1. self service ETL 
1. Need new report (that the existing data can't answer)
    1. Someone go and create a new data pipeline (what is data pipeline??)
    1. go back and forth - try to understand the data - stand it up somehow
    1. Done ? Ok. Now operationlize it. 
    1. How often do you need the data? 
    1. Create a workflow. Schedule / event based or Trigger based. 
    1. Done. Now keep monitoring it. 
    1. Business team - wants to do self service - they dont know what they want and would rather do it themselves 
    1. Allows them to claim victory even when they make very little progress. 





## Step by step no-code RAG application using Langflow.

1. https://www.youtube.com/watch?v=RWo4GDTZIsE
1. Langflow 
1. Langflow is a low-code tool for developers 
1. build powerful AI agents and workflows that can use any API, model, or database.
1. [langflow code](https://github.com/langflow-ai/langflow)
1. Langflow is a low-code app builder for RAG and multi-agent AI applications. It’s Python-based and agnostic to any model, API, or database.
1. [Langflow free cloud service](https://astra.datastax.com/signup?type=langflow)
1. [Welcome to Astra, kaun](https://astra.datastax.com/org/3e89fe60-0b63-4339-aca9-d4baaca01fb0)
1. [DataStax Langflow Docs](https://docs.datastax.com/en/langflow/index.html)

1. [OpenAI developer platform](https://platform.openai.com/docs/overview)
1. [Astra DB Serverless - database-as-a-service (DBaaS) - Apache Cassandra®, an open-source NoSQL distributed database](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html)
    1. Designed for vector search applications, such as Generative AI (GenAI) and semantic search
    1. can load vectors directly with your data 
    1. or, automatically generate vector embeddings with vectorize



## Youtube videos 

1. What are the infrastructure required. 
1. RODE - do you need that bada$$ mike? why? 
1. How do you show your screen and yet have yourself as an inset? 


## Augmenting LLMs Using Retrieval Augmented Generation by Nvidia 

1. Generative Pre-trained Transformer (GPT) 
1. LLM Fine Tuning vs. RAG 
1. Llama Index 
1. E5 large - embedding model 

## OpenAI / Swarm  

1. Swarm is an experimental, open-source Python framework from OpenAI 
1. that helps developers build, orchestrate, and deploy multi-agent systems:


## RAG vs Agentic RAG: A Comprehensive Guide 

1. https://www.analyticsvidhya.com/blog/2024/11/rag-vs-agentic-rag/

1. LLM without RAG 
1. The knowledge of RAG is constrained 
1. Large Language Model (LLM) on its own has a predefined training dataset that serves as its “internal knowledge.” 
1. You would like to add to its knowledge somehow 
1. Fine Tuning a LLM or a RAG 

1. **RAG (Retrieval-Augmented Generation)**
1. RAG is a framework LLM uses to get relevant, up-to-date, and context-specific information 
1. by combining retrieval and generation capabilities.



The process flow of an Agentic RAG System for handling user queries. Here’s a breakdown of each component:

## What the f**k is Agentic RAG? 

Agentic RAG refers to a more intelligent and dynamic Retrieval-Augmented Generation system where 
Agentic: The system works on its own, making decisions and taking actions depending on the situation.

1. an “agent” plays a key role in orchestrating processes. 
1. The agent intelligently determines which resources or databases are most relevant for a user’s query, 
1. making it capable of handling more complex, multi-tasking scenarios. 
It is an evolution from traditional RAG systems, 
offering greater adaptability and decision-making by incorporating additional logic or heuristics into the retrieval and response generation pipeline.
RAG (Retrieval-Augmented Generation): It mixes information from a knowledge base with the AI’s ability to create responses.



## What is Agentic Design Patterns?

1. https://www.analyticsvidhya.com/blog/2024/10/agentic-design-patterns/
1. The agentic design pattern is introduced as a solution for making LLMs more autonomous. 
1. Instead of just giving the model one prompt and expecting a final answer (like writing an essay in one go), 
1. an agent-like approach involves prompting the LLM multiple times, step by step. 
1. Each step refines the task, with the model improving its output iteratively.
1. When we prompt an LLM in zero-shot mode, it’s like asking someone to write a story in one go without revising. 
1. LLMs do well at this, but they can do even better. 
1. By using an agent-like workflow, we can prompt the LLM multiple times in steps. 
1. Each step builds on the previous one, refining the response. 
1. Think of it like asking the LLM to go over the essay multiple times, improving it with each pass.


## Example of usage of Agentic Design Pattern 

1. writing a code using Agentic workflow: 
Plan an outline for the code: Break down the task into smaller modules or functions.
Gather information and content: Research libraries, algorithms, or existing solutions. Do web searches or check the documentation if needed.
Write the first draft of the code: Implement the basic functionality, focusing on structure over perfection.
Review the code for inefficiencies or errors: Check for unnecessary code, bugs, or logic flaws.
Revise the code: Refactor, optimise, or add comments for clarity.
Rinse and repeat until the code is efficient and clean.


## What is the criteria for Retrieval ? 

## What is a Vector Database ? Why is it important for Agentic RAG ? 



## How does Agentic RAG system work? How does it handle user queries? 

1. The system receives a user query.
1. Does the query fit the criteria for retrieval (it is part of the vector database).

## Agent 

1. [What is an Agent?](https://theneuralmaze.substack.com/p/what-is-an-agent?utm_source=profile&utm_medium=reader2)
1. **Agentic Patterns**
    1. Tool use 
    1. Multi Agent 
    1. Planning 
    1. Reflection 

## Reflection Pattern 

1. https://theneuralmaze.substack.com/p/reflection-pattern-agents-that-think?utm_source=profile&utm_medium=reader2 

## Groq 

1. https://console.groq.com/docs/overview 
1. https://console.groq.com/docs/quickstart

