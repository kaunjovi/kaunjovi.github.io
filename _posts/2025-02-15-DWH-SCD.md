---
layout: post
title: Slowly Changing Dimensions (SCD)
categories: [data, datawarehouse, SCD] 
---


## Star Schema 

1. The star schema was introduced in 1996 by Ralph Kimball, 
1. in his book “The Data Warehouse Toolkit”. 
1. Star schema is a way to reduce the amount of data stored in a data warehouse, as well as improve query performance. 
1. Flash forward nearly thirty years, and the star schema is a staple of data warehouse design in thousands of modern data platforms.

1. In a star schema, data is stored as “fact” tables and “dimension” tables. 
1. Fact tables 
    1. transaction at a retail store, 
    1. a reservation for a guest at a hotel
        1. Dimension - Floor, number of beds, bathrooms for all rooms in the hotel 
    1. patient visits to a doctor
        1. Dimension - Patient information, such as address and phone number


## Slowly Changing Dimensions (SCD)

1. Method used to manage and track changes in the **dimensions data**. 

1. Type 0 - Data does not change at all - No syaappa at all. 
1. Type 1 - No history. Overwrite. Upsert. Update or Insert
    1. Address of customer. Dont need previous address. Just give me the latest. 
    1. Joining date of an employee. Not going to change for the duration of employment. 
    1. Amount of stuff sold in grocery store on a date. 
    1. Only latest data. Useful for dashboarding. Current state reporting and analysis? Great. 
    1. **issues** 
    1. If you wanted to do any historical data analysis e.g. change in pricing overtime, there is no scope 

1. Type 2 - Add a new row. 
    1. Keep history. New record each time value changes. Flag for the latest ? 
    1. Flag that the new row is current and the previous ones historical. 
    1. Or you could have a start_date and end_date to each row. The end_date for current rows are set far in future. 
        1. Why not leave the end_date empty for the current record ?? 

1. Type 3 - Add a new attribute 
    1. Keep the new value, but also the old value in the other column. 
    1. Can be used only in dimensions which change once (or twice) in it's lifetime. 
    1. So, there is a current_value and previous_val column to each row. 
    1. There is only value in current_value. The previous_val is empty. For most rows. For almost all of the time. 
    1. However, in the unlikely event of a value change, the current value moves to the previous_val column and the new value becomes the current_value. 
    1. Quite limiting. Have not seen it as yet. 

## Implementing SCD1 with Snowflake 

1. Create staging table with the new set of data. Or changed set of data. 

```SQL 
CREATE OR REPLACE TABLE stage_employees (
	employee_id INT,
	name VARCHAR,
	birthday DATE,
);

INSERT INTO stage_employees (
	employee_id,
	name,
	birthday
) VALUES (
	7,
	'P B',
	'1977-06-01',
);
```

1. MERGE it into the required dimension 

``` SQL
MERGE INTO table USING staging_table 
    on table.id = stage_table.id 
    -- Found a match. Update only. 
    WHEN MATCHED THEN UPDATE SET 
        table.value1 = stage_table.value1. 
    WHEN NOT MATCHED THEN INSERT ( 
        value1
    )  VALUES (
        stage_table.value1 
    )
```

1. What happens if I received only the changed columns. So, for the id that matches, I am expected to retain the data in most of the columns, but upadate the value in one or two columns that I received now. I did not receive a copy of the data that did not change. So, I can't update the value I got blindly. ?? 

## Implementing SCD2 with Snowflake 

``` SQL 
-- Update the old record 
MERGE INTO table USING stage_table 
    ON table.id = stage_table
    AND table.end_date = NULL 
WHEN MATCHED 
THEN UPDATE SET 
    table.end_date = stage_table.start_date 


-- CREATE A NEW RECORD 
MERGE INTO table USING stage_table 
    ON table.id = stage_table.id 
    AND table.end_date = NULL 
WHEN NOT MATCHED 
THEN INSERT (
    table.id, 
    table.col1, 
    table.start_date, 
    table.end_date 
)
VALUES (
    stage_table.id, 
    stage_table.col1, 
    stage_table.start_date, 
    state.end_date
)

```

## Implementing SCD3 with Snowflake 

```SQL 
CREATE TABLE stage_table (
    id INTEGER, 
    name VARCHAR, 
    day DATE 
)

INSERT INTO stage_table (
    id, 
    name, 
    day
)VALUES (
    7, 
    "BOND", 
    "01/07/1977"
); 

MERGE INTO table USING stage_table 
    ON table.id = stage_table.id 
WHEN MATCHED 
    -- check for a possible duplicate. 
    -- Update only if the data to be updated is different from the exsiting one 
    AND table.changed_value <> stage_table.changed_value
    THEN UPDATE SET 
        table.last_changed_value = table.changed_value 
        table.changed_value = stage_table.changed_value
WHEN NOT MATCHED
    -- The SCD 3 row will start with NULL previous value for the first time. 
    THEN INSERT (
        id, 
        name, 
        day
    )VALUES (        
        stage_table.id, 
        stage_table.name, 
        stage_table.changed_value, 
        NULL
    )

```

## SCD implementation issues
1. Inter batch - between same batch of data - find some way to de-dupe. Document. Review. Write a lot of emails and create a huge document trail. 
1. Intra batch - between different batches of data - check if the data being updated is actually different or it is the same duplicate record. 
1. Ensure Data Integrity 
1. Performance 
    1. Use constraints like the primary keys on the target table. 
    1. take advantage of micro-partitions 
    1. take advantage of data clustering 
    1. Check Query Plan and Query Profile 
    1. MERGE is worse than UPDATE and INSERT 
    1. SCD1 and SCD3 - are better performing than - SCD2. 
    1. Consider orchestrating using Apache Airflow. Mage, Prefect, Dagster. 



## Reference
1. [Datacamp](https://app.datacamp.com/learn/tutorials/mastering-slowly-changing-dimensions-scd)
1. [Geek for Geeks](https://www.geeksforgeeks.org/slowly-changing-dimensions/)
1. [Youtube](https://www.youtube.com/watch?v=-XjPvozl-So)


