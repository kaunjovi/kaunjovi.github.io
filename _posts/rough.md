

## Decision Tree 

1. A Decision tree is a flowchart-like tree structure, 
1. each internal node denotes a test on an attribute, 
1. branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. 
1. A tree can be ‚Äúlearned‚Äù by splitting the source set into subsets based on an attribute value test. 
1. This process is repeated on each derived subset in a recursive manner called recursive partitioning. 
1. The recursion is completed when the subset at a node all has the same value of the target variable, or 
1. when splitting no longer adds value to the predictions.

1. A decision tree is a black-box estimator (??)

## Classifier / Bagging classifier  

1. A Bagging classifier is an ensemble meta-estimator(??) 
1. that fits base classifiers (??) 
1. each on random subsets of the original dataset and 
1. then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. 
1. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (??) (e.g., a decision tree), 
1. by introducing randomization into its construction procedure and then making an ensemble(??) out of it.

1. Each base classifier is trained in parallel with a training set which is generated by 
1. randomly drawing, with replacement, N examples(or data) from the original training dataset, (?????)
1. where N is the size of the original training set. 
1. The training set for each of the base classifiers is independent of each other. 
1. Many of the original data may be repeated in the resulting training set while others may be left out.

1. Bagging reduces overfitting (variance) by averaging or voting, however, 
1. this leads to an increase in bias, which is compensated by the reduction in variance though.

1. Original data / split in parallel / bootstrapping + aggregating + classifier / feed back into Ensemble Classifier 


## Random Forest

1. Forest - it has multiple decision trees 
1. Random - the trees work on a random subset of data by row sampling and feature sampling. 

1. Every decision tree has high variance, 
1. but when we combine all of them together in parallel then the resultant variance is low
1. as each decision tree gets perfectly trained on that particular sample data and 
1. hence the output doesn‚Äôt depend on one decision tree but multiple decision trees. 

1. In the case of a classification problem, the final output is taken by using the majority voting classifier. 
1. In the case of a regression problem, the final output is the mean of all the outputs. 
1. This part is Aggregation.
¬†
1. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.
1. Random Forest has multiple decision trees as base learning models. 

1. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. 
1. This part is called Bootstrap.

## Boosted Trees 

1. [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/stable/tutorials/model.html#introduction-to-boosted-trees)
1. [Paper : Greedy Function Approximation: A Gradient Boosting Machine, by Friedman](https://jerryfriedman.su.domains/ftp/trebst.pdf) - sorry. OHT. 

## ensemble learning algorithms / XGBoost, RandomForest etc. 

1. Ensemble techniques in machine learning function much like seeking advice from multiple sources before making a significant decision, such as purchasing a car. 
1. Just as you wouldn‚Äôt rely solely on one opinion, ensemble models combine predictions from multiple base models to enhance overall performance. 
1. One popular method, majority voting, aggregates predictions to select the class label by majority. 

1. Ensemble learning is a machine learning technique that enhances accuracy and resilience in forecasting 
1. by merging predictions from multiple models. 
1. It aims to mitigate errors or biases that may exist in individual models by leveraging the collective intelligence of the ensemble.
1. The underlying concept behind ensemble learning is to combine the outputs of diverse models to create a more precise prediction. 
1. By considering multiple perspectives and utilizing the strengths of different models, 
1. ensemble learning improves the overall performance of the learning system. 
1. This approach not only enhances accuracy 
1. but also provides resilience against uncertainties in the data. 
1. By effectively merging predictions from multiple models, ensemble learning has proven to be a powerful tool in various domains, offering more robust and reliable forecasts.

1. How to make sense of forecasts from a large number of models. These are the basic Ensemble Techniques.  
    1. Max Voting (mode of all predictions) - classification problem - example? - 8 out of 10 are saying BUY. Might be BUY. 
    1. Averaging - regression problem - example(?)
        1. also, while calculating probablities of Clarssification Problem. 
    1. Weighted Averaging - average, but with unequal weights - 

1. There are some *advanced techniques* 
    1. Stacking 
    1. Blending 
    1. Bagging ( )
    1. Boosting 


1. **References** 
    1. [A Comprehensive Guide to Ensemble Learning](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)


## XGBoost (Extreme Gradient Boosting), a machine learning algorithm


1. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754)
    1. the original paper 
    1. needs some background to comprehend 



1. optimized distributed gradient boosting library designed for efficient and scalable training of machine learning models. (??)
1. combines the predictions of multiple weak models to produce a stronger prediction. 
1. XGBoost is known for 
    1. ability to handle large datasets with built-in support for parallel processing 
    1. ability to achieve state-of-the-art performance in many machine learning tasks such as classification and regression.
    1. efficient handling of missing values. dont need as much pre-processing 

1. proposed by the researchers at the University of Washington. 
1. It is a library written in C++ which optimizes the training for Gradient Boosting 
    1. **What is gradient boosing**?? 

1. [XGBoost at geeksforgeeks](https://www.geeksforgeeks.org/xgboost/)
1. [Getting started with XGBoost with Python](https://xgboost.readthedocs.io/en/stable/get_started.html)

```
from xgboost import XGBClassifier
# read data
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)
# create model instance
bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')
# fit model
bst.fit(X_train, y_train)
# make predictions
preds = bst.predict(X_test)
```


## Data Engineering / Data Pipeline vs ETL pipeline 

1. [Data Pipeline vs ETL Pipeline: What‚Äôs the Difference?](https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/)

## Data Engineering / What is an Extract, Transform and Load (ETL) Pipeline? 

1. [What is an ETL Pipeline?](https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/)

1. **Extracting data** from a variety of sources 
    1. the system ingests data from various heterogeneous sources, 
    1. such as business systems, applications, sensors, and databanks. 
    1. 
1. **Transform** 
    1. transforming the raw data into a format required by the end application.¬†
1. **Load** 
    1. The data is subsequently loaded into the target systems, 
    1. such as a cloud‚ÄØdata warehouse,‚ÄØdata mart, or a database 
    1. the transformed data is loaded into a target data warehouse or database. 
    1. it can be‚ÄØpublished as an API‚ÄØto be shared with stakeholders.¬†

## Data Engineering / What is Data Pipeline? 

A data pipeline refers to the steps involved in moving data from the source system to the target system. 
These steps include copying data, transferring it from an onsite location into the cloud, and combining it with other data sources. 
The main purpose of a data pipeline is to ensure that all these steps occur consistently to all data.
If managed astutely with‚ÄØdata pipeline tools, 
a data pipeline can offer companies access to consistent and well-structured datasets for analysis. 
Data engineers can consolidate information from numerous sources and use it purposefully by systematizing data transfer and transformation. 
For example, an‚ÄØAWS‚ÄØdata pipeline allows users to freely move data between AWS on-premises data and other storage resources.

Data pipelines are helpful for accurately fetching and analyzing data insights. 
The technology is helpful for individuals who store and rely on multiple siloed data sources, require real-time data analysis, or have their data stored on the cloud.¬†

For example, data pipeline tools can perform‚ÄØpredictive analysis‚ÄØto understand potential future trends. A production department can use predictive analytics to know when the raw material is likely to run out. Predictive analysis can also help forecast which supplier could cause delays. Using efficient data pipeline tools results in insights that can help the production department streamline its operations.



## Data Engineering / fixing slow ETL pipelines. 

1. https://www.linkedin.com/advice/1/your-etl-pipelines-slowing-down-data-processing-f5vcf?trk=cah2


Dynamic Scaling: Enable autoscaling on Databricks for resource efficiency.
Automate dynamic resource scaling with cloud options like AWS Lambda to adjust compute power based on load. 
Caching & Partitioning: Automate caching and partitioning to balance load.
Monitoring & Alerts: Use Azure Monitor/AWS CloudWatch for quick issue detection.
catch bottlenecks in real time (think Apache Spark or Airflow alerts üîî).
Predictive Optimization: Use ML to anticipate and prevent bottlenecks.
Incremental Loading: Load only new/modified data for efficiency.
Orchestration: Use Airflow to manage dependencies dynamically.
Testing: Automate regression tests to maintain performance.


For complex jobs, parallelize tasks üõ§Ô∏è, allowing data to flow faster and smoother. 
Use automated scheduling üï∞Ô∏è to optimize run times during low-traffic hours, and enable caching üóÑÔ∏è for frequently accessed data. 
Finally, keep pipelines tidy by automating cleanup tasks‚Äîlike a digital Marie Kondo üßπ! 

Watch data processing speed up as those bottlenecks melt away! 



## Business / Why is RecordKeeping a loss making business for T? 
## Business / Why is RecordKeeping a loss making business for T? 

1. [To cover losses elsewhere, TIAA pushes costly in-house products on retirement savers, whistleblower says](https://www.nbcnews.com/investigations/tiaa-pushes-costly-retirement-products-cover-losses-whistleblower-rcna161198)


## Business / How does the firm T make money? 

1. [read]()

1. TIAA‚Äôs business has two main parts: 
    1. First, it acts as recordkeeper for the retirement plans of institutions like 
        1. Cornell University, 
        1. the University of North Carolina System and 
        1. most of the nation‚Äôs 107 historically Black colleges and universities. 
    1. Second, TIAA earns money as an asset manager on 
        1. in-house mutual funds and 
        1. insurance products sold to investors 
        1. both inside and outside those (retirement) plans.        
        1. Annuities are contracts that promise to provide income for holders during their lives.

1. Clients 
    1. While most of TIAA‚Äôs clients are current or retired workers at nonprofit organizations, 
    1. TIAA also offers individual retirement accounts to investors online.

1. Most of the institutional accounts handled by TIAA 
1. allow participants to invest both in its own products and 
1. also in funds from outside firms like Vanguard or Fidelity. 

1. selling in-house products generates significantly greater profits to TIAA than sales of external offerings.

1. TIAA Traditional, its popular annuity, the company's engine, does not have an¬†‚Äúidentifiable ‚Äòexpense ratio‚Äô or ‚Äòfee,‚Äô‚Äù the company says. 
1. Chris¬†Tobe, an expert in pension investments and a former trustee of the $14 billion Kentucky Retirement Systems, 
1. said the industry consensus is TIAA makes around 1.2% on TIAA Traditional, 
1. far more than¬†other types of investments. 
1. Craig Parkin, the firm‚Äôs head of retirement advice and consulting, 
1. described TIAA Traditional as the company‚Äôs¬†‚Äúengine‚Äù¬†in the recording¬†of a sales meeting last fall.

1. The problem for clients arises when TIAA products promoted by its sales representatives or its advice tool 
1. cost more than its competitors‚Äô or perform poorly by comparison. 
1. An example is a TIAA real estate product the advice tool funnels clients to ‚Äî 
1. it has significantly underperformed¬†an index of real estate investment trusts 
1. in six of the past 10 years and is down 4.6% in 2024 while the index is up 4.2%.

1. When Rajotte made his presentation to TIAA employees, 
1. he cited Yale University‚Äôs retirement plans as an example of how TIAA compensates for its losses elsewhere by selling in-house products. 
1. At Yale, he said, ‚Äúwe‚Äôre more than making up for the loss in recordkeeping revenue by what we‚Äôre making in the product side.‚Äù

1. At a sales meeting  
1. [Thomas Rajotte](https://www.linkedin.com/in/tomrajotte/) TIAA's director of finance, told employees in a recorded presentation 
1. that the company could secure its future and offset its losses by selling in-house products to clients. 
1. "If they [clients] have Vanguard, we're not earning any money on the product and we're losing money on the recordkeeping," 
1. he said. "Where we make that up is on the product side."


1. Asked if Yale was concerned about TIAA‚Äôs in-house products being pushed on its participants to offset losses, the university said in a statement: 
1. ‚ÄúNeither Yale nor any of its plan fiduciaries has access to TIAA‚Äôs internal business discussions, nor is it Yale‚Äôs role to monitor TIAA.‚Äù
1. overseeing its retirement plans are fiduciaries who 
1. ‚Äúretain independent investment advisers, 
1. who are also fiduciaries to the plans, 
1. to provide expert assistance in identifying suitable investment funds and monitoring their performance. 
1. The plan fiduciaries periodically evaluate whether the investment fund fees that the plans pay TIAA are reasonable.‚Äù

1. If a financial firm advises customers to buy in-house investments that make it more money than other vehicles, 
1. it must disclose that fact. 
1. Ditto if a firm rewards sales representatives with higher compensation for directing clients into certain investments.
1. **is that a legal requirement**

1. In the 2021 SEC case, 
1. some TIAA training materials encouraged the firm‚Äôs wealth management advisers to **avoid discussing with clients**
1. the fees and expenses associated with shifting their low-cost employee retirement accounts into higher-cost managed accounts. 
1. TIAA misled clients by characterizing the recommendations to move into managed accounts as objective advice, according to the SEC. 
1. The activities occurred from January 2013 through March 2018; 
1. in accepting the settlement from TIAA, the SEC said it considered remedial actions the company took before and after the investigation, 
1. including changes to its disclosures, training, policies and compensation structure.

1. TIAA also tried to propel clients into making investment choices 
1. that would benefit the firm by citing the possibility that they would not have enough money to retire, the SEC said. 
1. ‚ÄúMaking the Client Feel the Pain,‚Äù was how TIAA‚Äôs scripted sales¬†materials¬†characterized the approach.¬†
1. TIAA said in 2017 that the language in those materials did not reflect the company‚Äôs values.

1. The recent¬†push at TIAA, 
1. according to the whistleblower complaint, 
1. differs from the 2021 case by focusing on **its financial consultants**, 
1. the employees who work directly with institutions like Brown University and Harvard.

1. At a meeting last fall, 
1. TIAA‚Äôs¬†Parkin¬†told financial consultants that 655,000 of the firm‚Äôs clients were ‚Äúoff track‚Äù for retirement.
1. They didn‚Äôt own enough in-house TIAA products, according to the complaint and a recording of Parkin‚Äôs presentation. 
1. Propelling these folks into TIAA Traditional, the annuity, would get them on track, he said. 
1. Even better, it would generate $10 billion for the firm, Parkin said.

1. Getting clients to use TIAA‚Äôs online advice tool was the means to this end, Parkin said on the recording. 
1. He also said TIAA‚Äôs CEO, Thasunda Brown Duckett, and others in the executive suite 
1. supported his strategy as a top priority to be pursued immediately.¬†
1. ‚ÄúThasunda loves her money,‚Äù Parkin said on the recording.

1. Like many online advice mechanisms, 
1. TIAA‚Äôs Retirement Advisor Field View Tool recommends clients balance their portfolio among various asset classes ‚Äî 
1. stocks, bonds, money markets and the like ‚Äî based on risk tolerance, age, income and other inputs. 
1. The tool divides investors into seven types ‚Äî very conservative to very aggressive ‚Äî 
1. then steers them to investment options in each asset class, internal documents show. 
1. Clients using the tool to change their portfolio must accept all its suggestions; 
1. a participant cannot accept one recommendation on stocks and reject another on real estate, for example.

1. Most of the asset classes available for users of the tool include an array of investment options that include 
1. both TIAA products and those offered by competitors.¬†
1. The company‚Äôs spokesman said¬†more than 2,500 non-TIAA funds are available to retirement plan sponsors for selection through its recordkeeping platform.
1. But in two asset classes, 
1. the tool‚Äôs recommendations are limited to TIAA products, 
1. One is the annuity that Parkin said generates significant profits to the firm; 
1. the other is the lackluster TIAA real estate product.

1. While previously consultants had been encouraged to discuss the tool‚Äôs recommendations with clients, 
1. now the employees are tasked with getting a certain number of clients to implement the tool‚Äôs advice. 
1. If a consultant persuades at least 170 customers to make changes using the tool, 
1. he or she earns credits toward bonuses accounting for one-quarter of their compensation.

1. Together, its top five executives received $46.2 million last year,  
1. Duckett, TIAA‚Äôs CEO, receiving $18.2 million, 
1. up from $17.5 million the prior year. 
1. Of the 18 companies TIAA considered to be peers for pay purposes last year, Duckett received more than CEOs at 10 of them, 
1. including Bank of New York Mellon, Equitable Holdings and T. Rowe Price.
1. TIAA, which operates without profit, 
1. paid its CEO more than the CEOs at most of the for-profit companies it¬†says are¬†peers.



1. LEAD - L ‚Äì Listen and be authentic, E ‚Äì Energise and inspire, A ‚Äì Align across the enterprise, D ‚Äì Develop others


1. Enterprise Data Architect certification. TOGAF? 

## Data Models 

1. [Conceptual Data Model](https://sparxsystems.com/enterprise_architect_user_guide/17.0/modeling_domains/conceptual_data_model__.html)
1. [Kimball‚Äôs Dimensional Data Modeling](https://www.holistics.io/books/setup-analytics/kimball-s-dimensional-data-modeling/)
1. [Difference between Kimball and Inmon](https://www.geeksforgeeks.org/difference-between-kimball-and-inmon/)
1. [The 3 brothers of Data Modelling: Kimball, Inmon & Data Vault](https://www.linkedin.com/pulse/3-brothers-data-modelling-kimball-inmon-vault-dnyanesh-bandbe/)
1. [Data Vault 2.0 is a modern approach to data warehousing](https://data-vault.com/what-is-data-vault-core-concept/)
1. [Data Warehouse Fundamentals ‚Äî Normalization, Data Modelling, Kimball Approach and Inmon Approach](https://medium.com/@khedekarpratik123/data-warehouse-fundamentals-normalization-data-modelling-kimball-approach-and-inmon-approach-3a325cea5ea1)

## Data Warehouse 
1. AWS Redshift, 
1. Google BigQuery, 
1. Snowflake


## Accordion / Executive Director - Data Management @ Hyd 
1. 4.3 / 4.1 - 
1. 1K employees 
1. financial consulting firm uniquely focused on private equity
1. Data & Analytics (Accordion | Data & Analytics)

1. Undergraduate degree (B.E/B.Tech.) from tier-1/tier-2 colleges are preferred. 
1. 12+ years of experience in related field
Experience in designing logical & physical data design architectures in various RDBMS (SQL Server, Oracle, MySQL etc.), Non-RDBMS (MongoDB, Cassandra etc.) and Data Warehouse (Azure Synapse, AWS Redshift, Google BigQuery, Snowflake etc.) environments.
Experience in designing logical and physical data design architectures in Oracle, MongoDB, and Snowflake. 
Currently focussing on wavewise migration to AWS and with an intention to migrate to Redshift in a year. 

Deep knowledge and implementation experience on Modern Data Warehouse principles using Kimball & Inmon Models or Data Vault including their application based on data quality requirements.

Implementation experiance of Modern Data Warehouse, Data Lakehouse, Data Product principles using AWS and Snowflake. 
Data Governance and Data Quality technical implementation experiance using Collibra. 
Also, expertise in working with Data owners, Data Stewards, Legal Risk and Compliance team. 


In-depth knowledge of AWS for solution design, development, and delivery.
Proven abilities to take on initiative, be innovative and drive it through completion.
Should have worked on Data Governance and led a team size of atleast 50+ resources 
Knowledge on integration of Salesforce and Siebel Data Source is a plus

Any Enterprise Data Architect certification will be an added advantage.

Partner with Business Heads (Marketing, Operations, Legal Risk and Compliance) to understand their domain, data and analytics requirements. 
Create comprehensive requirements, product roadmap to enable development of optimal Enterprise Data Platform. 
Align business requirements with technical architecture and provide technical leadership to development team for implementation. 
Translate business requirements into Data Products, Data Lakehouse architecture, including Data Governance, Data Quality, Security and other associated Non Functional Requirements (NFR).
Create elaborate Data Governance, Data Quality and Data management processes and procedures and work with Executive Commitee memmbers and Business Heads to roll out adoption. 
Collaborate with BU heads and internal project teams to implement data products and further Enterprise Data Platform adoption. 
Research and track the latest technical developments in the field of Data and AI. Also contribute to it by creating Patents, whitepapers etc. 




## JPMC / Applied AI ML Director @ Bglr 

tech leader ready to take their career to new heights. 

As an Applied AI ML Director at JPMorgan Chase within the 
Global Private Banking function's Technology Artificial Intelligence and Machine Learning (AIML) Team. 

You will provide deep engineering expertise and 

work across agile teams 
to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Leverage your deep expertise to consistently challenge the status quo, innovate for business impact, lead the strategic development behind new and existing products and technology portfolios, and remain at the forefront of industry trends, best practices, and technological advances

This role will focus on establishing and nurturing 
common capabilities, best practices, and reusable frameworks, 
creating a foundation for AI excellence that accelerates innovation and consistency across business functions

Establish and promote a library of common ML assets, 
reusable ML models, features stores, data pipelines, and standardized templates.
Lead efforts to create shared tools and platforms that streamline the end-to-end ML lifecycle across the organization.
Advise on the strategy and development of multiple products, applications, and technologies.
Lead advisor on the technical feasibility and business need for AIML use cases.
Drive adoption and integration of ML solutions across functions.
Promote experimentation and pilot programs to test new technologies and methodologies.
Liaison with firm wide AIML stakeholders.
Create novel code solutions and drives the development of new production code capabilities across teams and functions
Translate highly complex technical issues, trends, and approaches to leadership to drive the firm‚Äôs innovation and enable leaders to make strategic, well-informed decisions about technology advancements.
Influence across business, product, and technology teams and 
successfully manages senior stakeholder relationships.


Formal training or certification on AI/ML concepts 
10+ years applied experience. 
In addition, 5+ years of experience leading technologists to manage, anticipate and solve complex technical items within your domain of expertise.
Experience in Natural Language Processing including Large Language Models.
Experience in building, evaluating and deploying Machine Learning Models into Production.
Advanced proficiency in Python developmental toolsets. 
Design, coding, testing and debugging skills in Python.

## Merck Healthcare / Head of Data Engineering¬†@ Bglr 
1. 4.1 / 4.2 - 10k+ employee - 


## Senior Director of Engineering, Arkose Labs

1. Arkose Labs overview 4.1 51 to 200 Employees 



## Group Head of AI/ML, Barclays 

1. development of the AI/ML risk management framework and its embedment within the organisation.
1. appropriately defined risk-assessed approach to AI/ML oversight.
1. Falls under broader firms Control Framework.
1. Consumers of AI/ML risk management framework
1. AI/ML Owners, Quantitative Analytics Teams, Technology, Legal, Compliance, Validation Teams, etc.
1. enterprise-wide AI/ML model definition to identify AI/ML risks.
1. risk-based operating model for responsible AI/ML adoption.
1. capabilities that support AI/ML adoption and risk management.
1. Laws impacting AI/ML risk mgmt f/work - SR11-7, SS1/23, EU AI Act, NIST 




## AWS certifications. Menu Card. Which one to attack first. 
1. https://github.com/kaunjovi/kaunjovi.github.io/blob/main/_posts/aws-certifications.md


1. Foundational - No prior experience needed.
    1. Cloud Practitioner 
    1. AI Practitioner 
1. Associate
    1. [AWS Certified Machine Learning Engineer - Associate (MLA-C01)](https://d1.awsstatic.com/training-and-certification/docs-machine-learning-engineer-associate/AWS-Certified-Machine-Learning-Engineer-Associate_Exam-Guide.pdf)
        1. ML Engineer, 75 USD, 85 questions, 170 minutes, Amazon SageMaker and other ML engineering AWS services
        1. Your results for the exam are reported as a scaled score of 100‚Äì1,000. The minimum passing score is 720.
        1. Domain 1: Data Preparation for Machine Learning (ML) (28% of scored content)
        1. Domain 2: ML Model Development (26% of scored content)
        1. Domain 3: Deployment and Orchestration of ML Workflows (22% of scored content)
        1. Domain 4: ML Solution Monitoring, Maintenance, and Security (24% of scored 
        1. [25 hrs, 450 INR, Udemy](https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/?couponCode=NVDIN35)
        1. SageMaker, Bedrock, and AI Skills

    1. *Data Engineer - 150usd/9000inr - glue,data ingestion,athena,lambda,dynamobd,ml with sagemaker*
    1. *Solution Architect / Associate / AWS Certified Solutions Architect - Associate (SAA-C03)*
        1. [Solutions Architect - Knowledge Badge Readiness Path](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1044/solutions-architect-knowledge-badge-readiness-path)
        1. 50 hrs. Damn. Free. wow. 
        1. [Ultimate AWS Certified Solutions Architect Associate SAA-C03](         https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/?couponCode=LEARNNOWPLANS)
        1. 499. 10hrs ish. 
    1. Developer, SysOps Administrator 
1. Professional - 2 years of prior AWS Cloud experience recommended.
    1. Solution Architect / Professional
    1. DevOps Engineer 
1. Speciality 
    1. Machine Learning 
    1. Security, Advanced Network 

### resources
1. [AWS certification](https://aws.amazon.com/certification/)
1. AWS Certified Solutions Architect - Associate
1. https://skillbuilder.aws/exam-prep/solutions-architect-associate
1. [499 / Ultimate AWS Certified Solutions Architect Associate SAA-C03 by Udemy](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/?couponCode=NVDIN35)


## [AWS Certified Solutions Architect - Associate (SAA-C03)](https://explore.skillbuilder.aws/learn/lp/2198/Standard%2520Exam%2520Prep%2520Plan%253A%2520AWS%2520Certified%2520Solutions%2520Architect%2520-%2520Associate%2520%28SAA-C03%29) 

## Solutions Architect Role.
1. Have 1 or more years of hands-on experience designing cloud solutions that use AWS services.
1. If you do not have the experience outlined, consider starting with the Solutions Architect - Knowledge Badge Readiness Path.
1. The Standard Exam Prep Plan does not include all resources and courses recommended in the full 4-step plan. 
1. Subscribe to AWS Skill Builder to gain access to the Enhanced Exam Prep Plan, which includes labs, flashcards, additional exam-style questions, and a full-length assessment.

## Step 1: Get to know the exam with exam-style questions.
1. Time commitment: Up to 3 hours
1. We recommend covering this material in week 1 of your preparation.
1. Review the exam details page.
1. Review the exam guide.
1. Take AWS Certified Solutions Architect - Associate (SAA-C03) Official Practice Question Set to understand exam-style questions. 
1. This appears in Module 2 of the Exam Prep Enhanced Course.

## Step 2: Refresh your AWS knowledge and skills.

1. [AWS Well-Architected Framework](https://docs.aws.amazon.com/pdfs/wellarchitected/latest/framework/wellarchitected-framework.pdf)
1. AWS Well-Architected Framework
1. AWS Security Best Practices
1. FAQs
1. Amazon EC2
1. Amazon S3
1. Amazon VPC
1. Amazon Route 53
1. Amazon RDS
1. Amazon SQS

## [AWS Well-Architected Framework](https://docs.aws.amazon.com/pdfs/wellarchitected/latest/framework/wellarchitected-framework.pdf)

1. 970 pages. What ????? 
1. The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. 
By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable
systems in the cloud.

## Introduction


## [Welcome to the TOGAF¬Æ Standard, Version 9.2, a standard of The Open Group](https://pubs.opengroup.org/architecture/togaf9-doc/arch/)


## [AWS Well-Architected](https://aws.amazon.com/architecture/well-architected/?ref=wellarchitected-wp&wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&wa-lens-whitepapers.sort-order=desc&wa-guidance-whitepapers.sort-by=item.additionalFields.sortDate&wa-guidance-whitepapers.sort-order=desc)

## [AWS Well-Architected Framework](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html)

## [Operational Excellence Pillar - AWS Well-Architected Framework](https://docs.aws.amazon.com/wellarchitected/latest/operational-excellence-pillar/welcome.html)

1. The focus of this paper is the operational excellence pillar of the AWS Well-Architected Framework. 
It provides guidance to help you apply best practices in the design, delivery, and maintenance of AWS workloads.
The AWS Well-Architected Framework helps you understand the benefits and risks of decisions you make while building workloads on AWS.
By using the Framework you will learn operational and architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable workloads in the cloud. 
It provides a way to consistently measure your operations and architectures against best practices and identify areas for improvement.
We believe that having Well-Architected workloads that are designed with operations in mind greatly increases the likelihood of business success.

1. The framework is based on six pillars:
    1. Operational Excellence
    1. Security
    1. Reliability
    1. Performance Efficiency
    1. Cost Optimization
    1. Sustainability

This paper focuses on the operational excellence pillar and 
how to apply it as the foundation of your well-architected solutions. 
Operational excellence is challenging to achieve in environments where 
operations is perceived as a function isolated and 
distinct from the lines of business and development teams that it supports. 
By adopting the practices in this paper you can build architectures that provide 
insight to their status, are activated for effective and efficient operation and 
event response, and can continue to improve and support your business goals.

This paper is intended for those in technology roles, such as chief technology officers (CTOs), architects, developers, and operations team members. After reading this paper, you will understand AWS best practices and the strategies to use when designing cloud architectures for operational excellence. This paper does not provide implementation details or architectural patterns. However, it does include references to appropriate resources for this information.

## [AWS Technical Essentials Part 1](https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=2198)

## [What Is AWS?](https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=1044)

## Cloud computing deployment models

1. On prem / Cloud / Hybrid 

1. On-premises
1. hosted and maintained hardware such as compute, storage, and networking equipment in their own data centers. 
1. They often allocated entire infrastructure departments to take care of their data centers, 
1. which resulted in costly operations that made some workloads and experimentation impossible.¬†

Cloud
Cloud computing is the on-demand delivery of IT resources over the internet with primarily pay-as-you-go pricing. 
With cloud computing, companies do not have to manage and maintain their own hardware and data centers. 
Instead, companies like Amazon Web Services (AWS) own and maintain data centers and 
provide virtual data center technologies and services to companies and users over the internet.

Hybrid

A third option is a hybrid deployment. This type of deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment between the cloud and existing on-premises infrastructure connects cloud resources to internal systems to extend and grow an organization's infrastructure into the cloud.



## AWS / Data Centers and Availability zone (AZ)

1. https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=1044
1. Pictures on my laptop. Can get destroyed. Upload on AWS data center. 
1. Alien comes and destroys the AWS data center. 
1. AWS had thought about this and had backed up the AWS data center with another one 
1. using 
    1. redundant 
    1. high speed and 
    1. low latency connection. 
1. This is known as a AWS availability zone (AZ)
1. AZ has one or more data centers. With redundant power, network and connectivity. 



## AWS / Region 

1. https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=1044
1. A cluster of AZ using 
    1. redundant 
    1. high speed and 
    1. low latency connection. 
1. Is called a region. 
1. Region are scattered across the world and are named as such e.g. North virginia region 

1. The list of all regions across the world ???

## AWS / How to chose the region to save your information? 

1. Consider the following aspects. 
1. Compliance - are you constrained to keep data within some geography 
    1. Are you required to keep your data within UK? Choose *London region* and stop thinking anymore about it. 
    1. Are you in Canada? Might be pick the *Canada central* region. 
1. Latency - what kind of response times do you need? Is there a benefit of being closer to most of your users? 
    1. If your users are in Oregon, might be put the photos in the *Oregon Region* 
1. Service availability - Not every service is available everywhere. Choose your place. 
1. Pricing - Some region e.g. *Sao Paolo* has higher pricing, due to tax reasons. 


## AWS Global infrastructure
1. Global Edge Network, consists of 
    1. Edge Locations - Amazon CloudFront - 400+ edge locations are available currently. 
    1. Regional Edge Caches 

## Every action that you make in AWS is an API call that is authenticated and authorized.

1. AWS application program interface (API)
    1. AWS Management Console
    1. AWS command line interface (CLI) - can access CLI through AWS CloudShell, using console. 
    1. AWS Software Development Kits (SDK) - Python, Java, NodeJs 

1. AWS Management Console - Just getting started? Stick to this. 

1. Check out this python snippet for interacting with AWS ???

```python
import boto3
ec2 = boto3.client('ec2')
response = ec2.describe_instances()
print(response)
```

## Security and compliance are a shared responsibility between AWS and you.

1. https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=1044
1. Customer is responsible for Security *in the* cloud. 
    1. Customer data. Encrypt. Customer side. Server side. 
    1. OS. Firewall. 
    1. 
1. AWS is responsible for Security *of the* cloud. 
    1. Region. AZ. Compute. Storage. Database. Networking. etc. 
    1. AWS Global Infrastructure that cloud is running on. 
    1. AWS backbone / private fiber cables 
    1. Compute. Storage. Database. Network. 

## [Protecting the AWS Root User](https://explore.skillbuilder.aws/learn/course/1851/play/135839/aws-technical-essentials-part-1;lp=1044)

1. When you first access AWS, you begin with a single sign-in identity known as the root user.
1. Do everything that you can do to protect the root user. Strong password. Multi factor authentication. 





